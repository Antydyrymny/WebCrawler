<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <!-- CSS -->
        <link rel="stylesheet" href="./css/style.css" />
        <link rel="stylesheet" href="./css/spinner.css" />
        <!-- Fonts -->
        <link rel="preconnect" href="https://fonts.googleapis.com" />
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
        <link
            href="https://fonts.googleapis.com/css2?family=Montserrat&family=Roboto&family=Source+Code+Pro&display=swap"
            rel="stylesheet"
        />
        <!-- D3 CDN -->
        <script src="https://d3js.org/d3.v7.min.js"></script>
        <title>Web Graph</title>
    </head>
    <body>
        <div class="wrapper">
            <div class="content">
                <h1>Web crawler with graph visualization</h1>
                <p class="subheading">
                    Dynamically generating interactive website graphs using web crawling
                    and D3.js visualization
                </p>
                <div class="graph">
                    <div class="svg-wrapper">
                        <div id="spinner">
                            <div></div>
                            <div></div>
                            <div></div>
                            <div></div>
                            <div></div>
                            <div></div>
                            <div></div>
                            <div></div>
                        </div>
                        <svg></svg>
                    </div>
                </div>
                <div class="description graphDescription">
                    *Click on a node to further map the graph <br />
                    **Arrows show the direction of connection, two-ways connections are
                    shown as blue lines
                </div>
                <div class="form-wrapper">
                    <div class="form-container">
                        <form action="" id="webCrawlerQuerry">
                            <div>
                                <label for="query"
                                    >Enter a URL or domain to build its graph:</label
                                ><br />
                                <input
                                    type="text"
                                    id="query"
                                    name="query"
                                    placeholder="Example: https://www.example.com or example.com"
                                    autocomplete="url"
                                />
                            </div>
                        </form>
                    </div>
                </div>
                <div class="examplesGallery">
                    <div class="galleryItem" id="galleryItem1">
                        <img src="./src/assets/images/github.jpg" alt="Github graph" />
                    </div>
                    <div class="galleryItem" id="galleryItem2">
                        <img src="./src/assets/images/mdn.jpg" alt="MDN graph" />
                    </div>
                    <div class="galleryItem" id="galleryItem3">
                        <img src="./src/assets/images/google.jpg" alt="Google graph" />
                    </div>
                </div>
                <div class="description exampleDescription">
                    *Some example graphs, click to load
                </div>
                <article>
                    <h2>About</h2>
                    <p>
                        This project is a web-based platform that allows users to explore
                        the structures of websites through an interactive graph generated
                        using D3.js. The platform utilizes a simple web crawler script,
                        employing the breadth-first search algorithm to traverse websites
                        by mapping link elements in their HTML to collect data about their
                        interconnections and visualize it.
                    </p>
                    <p>
                        By entering a domain in the input field, you will prompt the
                        crawler script to explore and map the respective website, building
                        its graph. In the resulting graph, nodes represent individual
                        pages, while links between nodes signify their associations.
                    </p>
                    <p>
                        Here I will discuss the design and implementation of the platform,
                        the challenges faced during its development, and limitations
                        related to certain web technologies.
                    </p>
                    <h2>Web Crawler Overview</h2>
                    <p>
                        Upon entering a domain, its URL is sent to the server to handle
                        cross-origin requests, where the web crawler runs starting with
                        the root page.
                    </p>
                    <p>The code looks something like this:</p>
                    <pre>
<code>
<span class="keyword">async function</span> <span class="function">crawler</span>(<span class="variable">url</span>, <span class="variable">explored</span>) {
    <span class="statement">if</span> (!<span class="function">validateURL</span>(<span class="variable">url</span>)) <span class="statement">throw</span> <span class="keyword">new</span> Error('URL is invalid');

    <span class="comment">// Initialise map-like data structure to hold nodes and their connections</span>
    <span class="keyword">const</span> <span class="variable">treeRoot</span> = <span class="keyword">new</span> InnerNode(<span class="variable">url</span>);
    <span class="keyword">const</span> <span class="variable">toVisit</span> = [<span class="variable">treeRoot</span>];

    <span class="comment">// Breadth-first search</span>
    <span class="statement">while</span> (<span class="variable">toVisit</span>.length) {
        <span class="keyword">const</span> <span class="variable">currentlyVisiting</span> = <span class="variable">toVisit</span>.<span class="function">shift</span>();
        <span class="comment">// Additional stop condition - already explored or another domain</span>
        <span class="statement">if</span> (<span class="variable">explored</span>.<span class="function">has</span>(<span class="variable">currentlyVisiting</span>.url.href) || !<span class="variable">currentlyVisiting</span>.inner) <span class="statement">continue</span>;

        <span class="comment">// Process each site for links by building its DOM</span>
        <span class="keyword">const</span> <span class="variable">containedNodes</span> = <span class="statement">await</span> <span class="function">processUrl</span>(<span class="variable">currentlyVisiting</span>.url);
        <span class="variable">containedNodes</span>.<span class="function">forEach</span>((<span class="variable">node</span>) => {
            <span class="variable">currentlyVisiting</span>.connect(<span class="variable">node</span>);
        });
        <span class="variable">toVisit</span>.<span class="function">push</span>(...<span class="variable">containedNodes</span>);

        <span class="comment">// Mark as processed</span>
        <span class="variable">explored</span>.<span class="function">add</span>(<span class="variable">currentlyVisiting</span>.url.href);
        <span class="variable">currentlyVisiting</span>.explored = true;
    }
    <span class="statement">return</span> { <span class="variable">treeRoot</span>, <span class="variable">explored</span> };
}
</code>
</pre>

                    <p>
                        As each page is visited, relevant information, such as page URLs
                        and their associated links, is collected and stored in a data
                        structure suitable for the graph. This data is sent to the
                        front-end, processed, and visualized using D3.js to create the
                        interactive graph.
                    </p>
                    <h2>Graph Visualization with D3.js</h2>
                    <p>
                        D3.js provides a lot of useful tools for data visualization.
                        Specifically, for creating graphs, there is a force-simulation
                        API. Working with it involves the following steps:
                    </p>
                    <ol>
                        <li>
                            <strong>Creating a simulation</strong> from a collection of
                            data with respective arrays
                            <code class="inlineCode">nodes</code> and
                            <code class="inlineCode">links</code> using D3.js methods
                            <code class="inlineCode">d3.forceSimulation(data.nodes)</code
                            >,
                        </li>
                        <li>
                            <strong>Adding desired forces</strong> to it (fine-tuning
                            strength coefficients of said forces to achieve the preferred
                            look and feel of the graph proved to be a challenging
                            process). The most notable forces used in this project are:
                            <code class="inlineCode">'link'</code> - to connect the nodes,
                            <code class="inlineCode">'charge'</code> - to make them repel
                            from one another. There are others to position the graph and
                            make nodes of the same domain stick closer to each other.
                        </li>
                        <li>
                            <strong>Visualizing nodes and links</strong> by creating the
                            <code class="inlineCode">'line'</code> and
                            <code class="inlineCode">'circle'</code> selections,
                            associating them with respective data, and representing them
                            with SVG elements. Selections can later be updated when new
                            nodes and links are added. For instance, code for creating a
                            <code class="inlineCode">'line'</code> selection can look like
                            this:
                        </li>
                    </ol>
                    <pre>
<code>
<span class="keyword">const</span> <span class="variable">linkGroup</span> = <span class="variable">svg</span>.<span class="function">select</span>('g.links');
<span class="statement">if</span> (<span class="variable">linkGroup</span>.<span class="function">empty</span>()) <span class="variable">linkGroup</span> = <span class="variable">svg</span>.<span class="function">append</span>('g').<span class="function">attr</span>('class', 'links');
<span class="keyword">const</span> <span class="variable">links</span> = <span class="variable">linkGroup</span>.<span class="function">selectAll</span>('line')
    .<span class="function">data</span>(<span class="variable">data</span>.links, (<span class="variable">d</span>) => `${<span class="variable">d</span>.source.id}-${<span class="variable">d</span>.target.id}`)
    .<span class="function">enter</span>()
    .<span class="function">append</span>('line')...<span class="comment">// Add arrowheads, styling, etc.</span>
</code>
</pre>
                    <h2 class="noTopMargin">Challenges and Limitations</h2>
                    <p>
                        It is important to note that in this project, site connections are
                        found by extracting links from their HTML. While this approach
                        works in many cases, it may not suffice for pages that load
                        content dynamically, especially those utilizing frameworks like
                        React or Angular. To properly crawl JavaScript-generated websites,
                        it would require executing all of the page's scripts and
                        generating its HTML. This can be achieved with a headless browser,
                        but such a process is more computationally intensive and may not
                        be suitable for rendering graphs dynamically. Therefore, crawling
                        such websites falls outside the scope of this project, which
                        focuses on exploring static HTML-based websites.
                    </p>
                    <p>
                        Even without scraping JavaScript-generated websites, and using a
                        rather straightforward algorithm, there are still some caveats
                        that mostly come down to dealing with URLs.
                    </p>
                    <ol>
                        <li>
                            <strong>URLs need to be validated</strong> to ensure that the
                            crawler identifies and collects only relevant links within
                            websites. Non-website elements, such as email addresses,
                            telephone numbers, script files, etc., need to be excluded.
                        </li>
                        <li>
                            <strong>Resolving links</strong> can be tricky due to various
                            link types, including absolute, relative, and those within the
                            scope of the <code class="inlineCode">base</code> tag, which
                            can contain relative links themselves.
                        </li>
                        <li>
                            <strong>URL normalization</strong> is also important because
                            links on the pages can appear different but lead to the same
                            resource. While it's possible to send a
                            <code class="inlineCode">HEAD</code> request to retrieve the
                            final URL of each page, doing so for all nodes can be
                            time-consuming, which may not be suitable for rendering graphs
                            on the fly. Instead, I made the decision to normalize them on
                            the server by removing trailing slashes, enforcing the
                            protocol and subdomain, handling casing, and sorting query
                            parameters. Accounting for all these scenarios, although it
                            may not resolve cases where there is a redirect, still results
                            in a quite consistent representation of URLs in the graph.
                        </li>
                    </ol>
                    <p>
                        In any case, it was an interesting side project to work on. If you
                        want to play with it, customize the look of the graph, or add some
                        functionality, you can find the project's source code on the
                        <a class="myLink" href="https://github.com/Antydyrymny/WebCrawler"
                            >GitHub page</a
                        >. Feel free to check it out and have some fun experimenting with
                        the web crawler and the graph!
                    </p>
                </article>
            </div>
        </div>
        <!-- Main script -->
        <script type="module" src="./src/js/main.js"></script>
    </body>
</html>
